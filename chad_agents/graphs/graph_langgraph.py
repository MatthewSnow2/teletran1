"""LangGraph State Machine: Initialize → Plan → Execute → Reflect → Finalize

Implements autonomous agent workflow with dual-LLM routing:
- Claude: Planning and reflection (technical reasoning)
- ChatGPT-5: User notifications and summaries
"""

import asyncio
import json
import re
from datetime import datetime
from typing import Any, Literal, TypedDict

from langgraph.graph import StateGraph, END

from chad_llm import LLMRouter, TaskType
from chad_tools.registry import ToolRegistry


# ============================================================================
# STATE SCHEMA
# ============================================================================


class AgentState(TypedDict):
    """State passed between LangGraph nodes."""

    # Execution metadata
    run_id: str
    actor: str
    goal: str
    autonomy_level: Literal["L0", "L1", "L2", "L3"]
    dry_run: bool

    # Planning
    plan: list[dict]  # Generated by Claude
    current_step: int
    max_steps: int

    # Execution
    executed_steps: list[dict]
    working_memory: dict  # Results from previous steps

    # LLM context
    messages: list  # Conversation history
    llm_calls: int  # Track usage

    # Results
    final_result: dict | None
    status: Literal["pending", "running", "completed", "failed"]
    error: str | None

    # Artifacts
    artifacts: list[dict]  # Created pages, files, etc.


# ============================================================================
# NODE IMPLEMENTATIONS
# ============================================================================


async def initialize_node(state: AgentState) -> AgentState:
    """Initialize execution context.

    Sets up initial state and prepares for planning.
    """
    state["status"] = "running"
    state["current_step"] = 0
    state["executed_steps"] = []
    state["working_memory"] = {}
    state["artifacts"] = []
    state["llm_calls"] = 0
    state["messages"] = []
    state["error"] = None

    # Add initialization message
    state["messages"].append({
        "role": "system",
        "content": f"Initializing agent workflow for goal: {state['goal']}"
    })

    return state


async def plan_node(state: AgentState, llm_router: LLMRouter) -> AgentState:
    """Generate execution plan using Claude.

    Args:
        state: Current agent state
        llm_router: LLM router for dual-LLM access

    Returns:
        Updated state with plan
    """
    # Check if replanning or first plan
    is_replan = len(state["executed_steps"]) > 0

    if is_replan:
        prompt = f"""
You are Chad, an autonomous knowledge agent.

Goal: {state["goal"]}

Previous execution:
{json.dumps(state["executed_steps"], indent=2)}

The previous plan didn't fully achieve the goal. Analyze what went wrong and create a revised plan.

Available Tools:
1. notion.search(query, max_results) - Search workspace for pages
2. notion.pages.read(page_id) - Read full page content as markdown
3. notion.pages.create(parent_id, title, content_markdown, icon_emoji) - Create new pages
4. notion.databases.query(database_id, filter_conditions) - Query databases

Output as JSON with this structure:
{{
  "steps": [
    {{
      "step_number": 1,
      "tool": "notion.search",
      "input": {{"query": "", "max_results": 100}},
      "purpose": "Discover all pages in workspace",
      "expected_output": "List of page IDs and titles"
    }}
  ],
  "reasoning": "Why this plan will work",
  "expected_outcome": "What success looks like"
}}
"""
    else:
        prompt = f"""
You are Chad, an autonomous knowledge agent with access to Notion.

Goal: {state["goal"]}

Available Tools:
1. notion.search(query, max_results) - Search workspace for pages
2. notion.pages.read(page_id) - Read full page content as markdown
3. notion.pages.create(parent_id, title, content_markdown, icon_emoji) - Create new pages
4. notion.databases.query(database_id, filter_conditions) - Query databases

Create a step-by-step execution plan. For each step:
- Specify which tool to use
- Define exact inputs (use {{{{step_N_result.field}}}} for data from previous steps)
- Explain what the step accomplishes

IMPORTANT: Keep plan under {state["max_steps"]} steps.

Output as JSON with this structure:
{{
  "steps": [
    {{
      "step_number": 1,
      "tool": "notion.search",
      "input": {{"query": "", "max_results": 100}},
      "purpose": "Discover all pages in workspace",
      "expected_output": "List of page IDs and titles"
    }}
  ],
  "reasoning": "Why this plan will achieve the goal",
  "expected_outcome": "What success looks like"
}}
"""

    # Plan schema for structured output
    plan_schema = {
        "type": "object",
        "properties": {
            "steps": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "step_number": {"type": "integer"},
                        "tool": {"type": "string"},
                        "input": {"type": "object"},
                        "purpose": {"type": "string"},
                        "expected_output": {"type": "string"},
                    },
                    "required": ["step_number", "tool", "input", "purpose"],
                },
            },
            "reasoning": {"type": "string"},
            "expected_outcome": {"type": "string"},
        },
        "required": ["steps", "reasoning", "expected_outcome"],
    }

    # Call Claude for planning
    try:
        response, model = await llm_router.generate_json(
            prompt=prompt,
            schema=plan_schema,
            task_type=TaskType.PLANNING,
            temperature=0.3,  # Lower temp for deterministic planning
        )

        state["plan"] = response["steps"]
        state["llm_calls"] += 1
        state["messages"].append({
            "role": "assistant",
            "content": f"[{model}] Plan created with {len(response['steps'])} steps. Reasoning: {response['reasoning']}"
        })

    except Exception as e:
        state["error"] = f"Planning failed: {str(e)}"
        state["status"] = "failed"
        state["messages"].append({
            "role": "system",
            "content": f"Error during planning: {str(e)}"
        })

    return state


async def execute_tool_node(
    state: AgentState,
    tool_registry: ToolRegistry
) -> AgentState:
    """Execute current step's tool.

    Args:
        state: Current agent state
        tool_registry: Tool registry for accessing tools

    Returns:
        Updated state with execution results
    """
    step_index = state["current_step"]

    # Check if we've completed all steps
    if step_index >= len(state["plan"]):
        return state

    step = state["plan"][step_index]

    # Resolve inputs from working memory (e.g., {{step_1_result.pages}})
    resolved_input = _resolve_template_inputs(
        step["input"],
        state["working_memory"]
    )

    # Get tool from registry
    tool_name = step["tool"]

    try:
        tool = tool_registry.get(tool_name)

        # Execute tool
        result = await tool.execute(
            ctx={
                "actor": state["actor"],
                "run_id": state["run_id"],
                "dry_run": state["dry_run"],
            },
            input_data=resolved_input
        )

        # Store result in executed steps
        state["executed_steps"].append({
            "step": step_index + 1,
            "tool": tool_name,
            "input": resolved_input,
            "output": result,
            "status": "success",
            "timestamp": datetime.utcnow().isoformat(),
        })

        # Store in working memory for next steps
        state["working_memory"][f"step_{step_index + 1}_result"] = result

        # Track artifacts (created pages, etc.)
        if "url" in result:
            state["artifacts"].append({
                "type": "notion_page" if "notion" in tool_name else "artifact",
                "url": result.get("url"),
                "title": result.get("title", "Untitled"),
                "created_at": datetime.utcnow().isoformat(),
            })

        state["messages"].append({
            "role": "system",
            "content": f"✅ Step {step_index + 1}/{len(state['plan'])}: {step['purpose']} - Success"
        })

    except Exception as e:
        # Handle tool execution error
        state["executed_steps"].append({
            "step": step_index + 1,
            "tool": tool_name,
            "input": resolved_input,
            "error": str(e),
            "status": "failed",
            "timestamp": datetime.utcnow().isoformat(),
        })

        state["messages"].append({
            "role": "system",
            "content": f"❌ Step {step_index + 1}/{len(state['plan'])}: {step['purpose']} - Failed: {str(e)}"
        })

    # Increment step counter
    state["current_step"] += 1

    return state


async def reflect_node(state: AgentState, llm_router: LLMRouter) -> AgentState:
    """Evaluate progress using Claude.

    Args:
        state: Current agent state
        llm_router: LLM router for dual-LLM access

    Returns:
        Updated state with reflection results
    """
    prompt = f"""
You are Chad, evaluating the progress of an autonomous task.

Goal: {state["goal"]}

Original Plan:
{json.dumps(state["plan"], indent=2)}

Executed Steps:
{json.dumps(state["executed_steps"], indent=2)}

Working Memory:
{json.dumps(state["working_memory"], indent=2)}

Analyze:
1. Has the goal been achieved?
2. Are there any errors that require replanning?
3. Should we continue with the plan or adjust?

Output as JSON:
{{
  "goal_achieved": true/false,
  "next_action": "continue" | "replan" | "done" | "failed",
  "reasoning": "Your analysis of what happened",
  "issues": ["List any problems encountered"],
  "suggestions": ["Suggestions for next steps if replanning"]
}}
"""

    reflection_schema = {
        "type": "object",
        "properties": {
            "goal_achieved": {"type": "boolean"},
            "next_action": {"type": "string", "enum": ["continue", "replan", "done", "failed"]},
            "reasoning": {"type": "string"},
            "issues": {"type": "array", "items": {"type": "string"}},
            "suggestions": {"type": "array", "items": {"type": "string"}},
        },
        "required": ["goal_achieved", "next_action", "reasoning"],
    }

    try:
        reflection, model = await llm_router.generate_json(
            prompt=prompt,
            schema=reflection_schema,
            task_type=TaskType.REFLECTION,
            temperature=0.4,
        )

        state["llm_calls"] += 1
        state["messages"].append({
            "role": "assistant",
            "content": f"[{model}] Reflection: {reflection['reasoning']}"
        })

        # Store reflection in working memory
        state["working_memory"]["reflection"] = reflection

    except Exception as e:
        state["error"] = f"Reflection failed: {str(e)}"
        state["messages"].append({
            "role": "system",
            "content": f"Error during reflection: {str(e)}"
        })

    return state


async def finalize_node(state: AgentState, llm_router: LLMRouter) -> AgentState:
    """Wrap up execution and notify user.

    Args:
        state: Current agent state
        llm_router: LLM router for dual-LLM access

    Returns:
        Final state with user notification
    """
    # Determine final status
    reflection = state["working_memory"].get("reflection", {})
    goal_achieved = reflection.get("goal_achieved", False)

    if state["error"]:
        state["status"] = "failed"
    elif goal_achieved:
        state["status"] = "completed"
    else:
        state["status"] = "completed"  # Partial completion

    # Generate user-friendly notification with ChatGPT-5
    prompt = f"""
Generate a friendly notification message for this workflow result:

Goal: {state["goal"]}
Status: {state["status"]}
Steps executed: {len(state["executed_steps"])}
Artifacts created: {len(state["artifacts"])}
LLM calls: {state["llm_calls"]}

Artifacts:
{json.dumps(state["artifacts"], indent=2)}

Write a concise, upbeat notification (2-3 sentences).
Include links to any artifacts created.
"""

    try:
        notification, model = await llm_router.generate(
            prompt=prompt,
            task_type=TaskType.USER_RESPONSE,
            temperature=0.7,
        )

        state["llm_calls"] += 1

        # Build final result
        state["final_result"] = {
            "run_id": state["run_id"],
            "status": state["status"],
            "goal": state["goal"],
            "steps_executed": len(state["executed_steps"]),
            "artifacts": state["artifacts"],
            "llm_calls": state["llm_calls"],
            "notification": notification,
            "error": state["error"],
        }

        state["messages"].append({
            "role": "assistant",
            "content": f"[{model}] {notification}"
        })

    except Exception as e:
        # Fallback notification if ChatGPT fails
        state["final_result"] = {
            "run_id": state["run_id"],
            "status": state["status"],
            "goal": state["goal"],
            "steps_executed": len(state["executed_steps"]),
            "artifacts": state["artifacts"],
            "llm_calls": state["llm_calls"],
            "notification": f"Workflow {state['status']}. {len(state['artifacts'])} artifacts created.",
            "error": state["error"] or str(e),
        }

    return state


# ============================================================================
# ROUTING FUNCTIONS
# ============================================================================


def decide_after_execution(state: AgentState) -> str:
    """Decide next step after tool execution.

    Returns:
        "reflect" if more steps remain, "done" if all steps complete, "error" if failed
    """
    # Check for errors
    if state["error"]:
        return "error"

    # Check if all steps executed
    if state["current_step"] >= len(state["plan"]):
        return "done"

    # Check if max steps exceeded
    if state["current_step"] >= state["max_steps"]:
        return "done"

    # Continue to reflection
    return "reflect"


def decide_after_reflection(state: AgentState) -> str:
    """Decide next step after reflection.

    Returns:
        "continue", "replan", or "done"
    """
    reflection = state["working_memory"].get("reflection", {})
    next_action = reflection.get("next_action", "continue")

    # Map reflection decision
    if next_action == "done" or reflection.get("goal_achieved"):
        return "done"
    elif next_action == "replan":
        return "replan"
    elif next_action == "failed":
        state["error"] = "Reflection determined workflow failed"
        return "done"
    else:
        # Continue with next step
        return "continue"


# ============================================================================
# GRAPH FACTORY
# ============================================================================


def create_knowledge_organization_graph(
    llm_router: LLMRouter,
    tool_registry: ToolRegistry,
) -> StateGraph:
    """Create LangGraph for knowledge organization workflow.

    Args:
        llm_router: Dual-LLM router
        tool_registry: Tool registry with Notion adapters

    Returns:
        Compiled StateGraph
    """
    # Create async wrapper functions (Python doesn't support async lambda)
    async def plan_wrapper(state: AgentState) -> AgentState:
        return await plan_node(state, llm_router)

    async def execute_tool_wrapper(state: AgentState) -> AgentState:
        return await execute_tool_node(state, tool_registry)

    async def reflect_wrapper(state: AgentState) -> AgentState:
        return await reflect_node(state, llm_router)

    async def finalize_wrapper(state: AgentState) -> AgentState:
        return await finalize_node(state, llm_router)

    graph = StateGraph(AgentState)

    # Define nodes with async wrappers
    graph.add_node("initialize", initialize_node)
    graph.add_node("plan", plan_wrapper)
    graph.add_node("execute_tool", execute_tool_wrapper)
    graph.add_node("reflect", reflect_wrapper)
    graph.add_node("finalize", finalize_wrapper)

    # Entry point
    graph.set_entry_point("initialize")

    # Linear flow: initialize → plan
    graph.add_edge("initialize", "plan")
    graph.add_edge("plan", "execute_tool")

    # Conditional routing after execution
    graph.add_conditional_edges(
        "execute_tool",
        decide_after_execution,
        {
            "reflect": "reflect",
            "done": "finalize",
            "error": "finalize"
        }
    )

    # Conditional routing after reflection
    graph.add_conditional_edges(
        "reflect",
        decide_after_reflection,
        {
            "continue": "execute_tool",  # Next step
            "replan": "plan",  # Adjust plan
            "done": "finalize"  # Goal achieved
        }
    )

    # End
    graph.add_edge("finalize", END)

    return graph.compile()


# ============================================================================
# MAIN EXECUTION FUNCTION
# ============================================================================


async def execute_agent_loop(
    run_id: str,
    goal: str,
    context: dict[str, Any],
    autonomy_level: str,
    dry_run: bool,
    max_steps: int,
    llm_router: LLMRouter,
    tool_registry: ToolRegistry,
) -> dict:
    """Execute LangGraph agent loop.

    Args:
        run_id: Unique run identifier
        goal: User's goal
        context: Execution context (actor, etc.)
        autonomy_level: L0, L1, L2, or L3
        dry_run: Whether to run in dry-run mode
        max_steps: Maximum steps to execute
        llm_router: Dual-LLM router
        tool_registry: Tool registry

    Returns:
        Final execution result
    """
    # Initialize state
    initial_state: AgentState = {
        "run_id": run_id,
        "actor": context.get("actor", "unknown"),
        "goal": goal,
        "autonomy_level": autonomy_level,
        "dry_run": dry_run,
        "plan": [],
        "current_step": 0,
        "max_steps": max_steps,
        "executed_steps": [],
        "working_memory": {},
        "messages": [],
        "llm_calls": 0,
        "final_result": None,
        "status": "pending",
        "error": None,
        "artifacts": [],
    }

    # Create graph
    graph = create_knowledge_organization_graph(llm_router, tool_registry)

    # Execute graph
    try:
        final_state = await graph.ainvoke(initial_state)
        return final_state["final_result"]

    except Exception as e:
        # Handle graph execution error
        return {
            "run_id": run_id,
            "status": "failed",
            "goal": goal,
            "steps_executed": 0,
            "artifacts": [],
            "llm_calls": 0,
            "notification": f"Workflow failed: {str(e)}",
            "error": str(e),
        }


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================


def _resolve_template_inputs(
    input_data: dict[str, Any],
    working_memory: dict[str, Any]
) -> dict[str, Any]:
    """Resolve template variables in inputs.

    Example:
        input_data = {"page_id": "{{step_1_result.pages[0].id}}"}
        working_memory = {"step_1_result": {"pages": [{"id": "abc123"}]}}
        → {"page_id": "abc123"}

    Args:
        input_data: Input dict with possible template variables
        working_memory: Working memory with previous step results

    Returns:
        Resolved input dict
    """
    resolved = {}

    for key, value in input_data.items():
        if isinstance(value, str) and "{{" in value and "}}" in value:
            # Extract template variable
            match = re.search(r"\{\{(.+?)\}\}", value)
            if match:
                template = match.group(1).strip()
                resolved_value = _resolve_path(template, working_memory)
                resolved[key] = resolved_value if resolved_value is not None else value
            else:
                resolved[key] = value
        else:
            resolved[key] = value

    return resolved


def _resolve_path(path: str, data: dict[str, Any]) -> Any:
    """Resolve dot-notation path in data.

    Example:
        path = "step_1_result.pages[0].id"
        data = {"step_1_result": {"pages": [{"id": "abc123"}]}}
        → "abc123"
    """
    parts = path.replace("[", ".").replace("]", "").split(".")
    current = data

    for part in parts:
        if isinstance(current, dict):
            current = current.get(part)
        elif isinstance(current, list):
            try:
                index = int(part)
                current = current[index]
            except (ValueError, IndexError):
                return None
        else:
            return None

        if current is None:
            return None

    return current
